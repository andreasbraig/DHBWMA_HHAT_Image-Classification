\chapter{Softwaretests} \label{chap:softwaretests}

In diesem Kapitel wird auf die Evaluierungs- und Testvorgäne für die Sicherstellung der Güte der entwickelten Software eingegangen.
Da es sich bei der entwickelten Software um ein Machine-Learning-Modell handelt, wird in diesem Kapitel auf die Evaluierung des Modells eingegangen. Die Funktionen des interpretierten Python Skripts entwickeln keine neuen Algorithmen, die auf ihre Fehleranfälligkeit oder Korrektheit getestet werden müssen.

\section{Metriken zur Evaluierung} \label{sec:metriken}

Um zu verstehen wie ein Machine Learning Modell evaluiert werden kann ist es zunächst wichtig die Arten von Kriterien nachzuvollzihen, die für die binäre Klassifikation notwendig sind. Im Folgenden wird allgemein von Instanzen gesprochen, es handelt sich dabei um die Bilder, der PCBs die klassifiziert werden sollen.

\begin{itemize}
    \item \textbf{True Positive (TP)}: Die Anzahl der korrekt klassifizierten positiven Instanzen.
    \item \textbf{True Negative (TN)}: Die Anzahl der korrekt klassifizierten negativen Instanzen.
    \item \textbf{False Positive (FP)}: Die Anzahl der falsch klassifizierten positiven Instanzen.
    \item \textbf{False Negative (FN)}: Die Anzahl der falsch klassifizierten negativen Instanzen.
\end{itemize}

Diese wesentlichen Typen teilen die Klassifizierten Daten nach dem Test in vier diskrete Kategorien ein. Diese Kategorien bilden die 
in \autoref{sec:confusionmatrix} beschriebene Confusion Matrix. Mit Ihnen können aber auch die Einfacheren Werte Loss und Accuracy berechnet werden. Im Folgenden wird die Mathematik hinter den Methoden vorgestellt.

\subsection{Accuracy und Loss} \label{sec:accuracy}

Die Accuracy ist eine der einfachsten Metriken zur Evaluierung eines Machine-Learning-Modells. Sie gibt an, wie viele der Instanzen korrekt klassifiziert wurden. Die Accuracy wird vereinfacht wie folgt berechnet:

\begin{equation}
    \text{Accuracy} = \frac{n_{\text{richtig klassifiziert}}}{n_{\text{gesamt}}}
\end{equation}

Bezug auf die in \autoref{sec:metriken} beschriebenen Kategorien, kann die Accuracy wie folgt berechnet werden \cite{ai_wiki_accuracy_2019}:

\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} 
\end{equation}

Die Accuracy wird typischerweise in Prozent angegeben und liegt zwischen 0 und 100\%. Sie ermöglicht eine schnelle Einschätzung der Modellgüte. Allerdings kann die Accuracy irreführend sein, da sie die Anzahl der falsch klassifizierten Instanzen nicht berücksichtigt. Ein Modell, das alle Instanzen als negativ klassifiziert, könnte eine hohe Accuracy aufweisen, obwohl es nicht leistungsfähig ist.

Dennoch ist die Accuracy einfacher zu Interpretieren als der hier vorgestellte Loss. Der Loss ist eine Metrik, die die Güte eines Modells anhand der Wahrscheinlichkeiten der Klassifikationen bewertet. 
Für binäre Klassifikationen wird der Binary Crossentropy Loss verwendet, der wie folgt berechnet wird 
\cite{ai_wiki_accuracy_2019}:

\begin{equation}
    \text{Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
    \label{eq:binary_crossentropy}
\end{equation}

\begin{tabular}{ll}
    \hline
    \textbf{Symbol} & \textbf{Bedeutung} \\
    \hline
    $n$ & Anzahl der Trainingsbeispiele \\
    $k$ & Anzahl der Klassen (bei Mehrklassen-Klassifikation) \\
    $y_i$ & Wahre Klasse (0 oder 1) für das $i$-te Beispiel \\
    $\hat{y}_i$ & Vorhergesagte Wahrscheinlichkeit für Klasse 1 \\
    \hline
\end{tabular}

In jeder Epoche des Trainings werden beide Werte berechnet und stellen so die Verbesserung des Modells dar. Der Loss wird dabei minimiert, während die Accuracy maximiert wird. Bereits in der Letzten Studienarbeit wurden die geteste Modelle anhand dieser Metriken evaluiert.

\subsection{Confusion Matrix und F1 Score} \label{sec:confusionmatrix}

Eine aussagekräftigere Methode zur Evaluierung ist der F1 Score. Um diesen nachzuvollziehen ist zunächst eine aufschlüsselung der Klassifizierten Daten notwendig. Die Confusion Matrix ist eine Tabelle, die die Anzahl der korrekten und falschen Klassifikationen für jede Klasse anzeigt. Die Confusion Matrix hat die folgende Form \cite{lipton_thresholding_2014}:

\[
\begin{array}{c|cc}
    & \textbf{Vorhergesagt: Positiv} & \textbf{Vorhergesagt: Negativ} \\
    \hline
    \textbf{Tatsächlich Positiv} & TP & FN \\
    \textbf{Tatsächlich Negativ} & FP & TN
\end{array}
\]

Hier sind die Werte TP, FP, TN und FN die in \autoref{sec:metriken} bereits eingeführt wurden wieder zu finden. Auf der Hauptdiagonale dieser $2x2$ Matrix befinden sich die korrekt klassifizierten Instanzen, während auf der nebendiagonale die falsch klassifizierten Instanzen zu finden sind. 

Für jeden Evaluierungsaufruf lässt sich diese Matrix bestimmen. Vorteilhaft an dieser Darstellung ist, dass die Möglichtkeit besteht dieses Modell auf nicht binäre Klassifikation zu erweitern. 

Aus dieser Matrix lassen sich nun weitere Metriken ableiten. Eine davon ist der F1 Score. Der F1 Score ist das harmonische Mittel zwischen Precision und Recall. Precision gibt an, wie viele der als positiv klassifizierten Instanzen tatsächlich positiv sind, während Recall angibt, wie viele der tatsächlich positiven Instanzen korrekt klassifiziert wurden. Der F1 Score wird wie folgt berechnet \cite{lipton_thresholding_2014}:

\begin{equation}
    \text{F1} = \frac{2 TP}{2 TP + FP + FN }
\end{equation}

Beispielhafte Confusion Matritzen werden in \autoref{sec:reevaluierung} vorgestellt.

\section{Reevaluierung des Modells} \label{sec:reevaluierung}

In diesem Kapitel soll das Ergebnismodell der letzten Studienarbeit, MobileNet V1 erneut Evaluiert werden. Diesmal mithilfe der neuen Metrinken (\autoref{sec:confusionmatrix}) und anhand eines neuen Datensatzes von PCBs (siehe \autoref{subsec:neuer_datzensatz}).

Für diese Reevaluierung wurden mehrere Modelle getestet. In dieser Arbeit werden drei Vertreter dieser Testreihe vorgestellt. Die Ergebnisse der Reevaluierung sind in \autoref{fig:vergleichsdiagramme} dargestellt.
Da es sich bei Mobilnet V1 und MobilenetV3 um vortrainierte Modelle von Tensorflow handelt, soll auch ein selbstgeschriebenes Modell evaluiert werden. Dieses Modell "Custom \ac{CNN}"  wurde im Rahmen der Vorlesung Bildverarbeitung entwickelt und fließt mit in diese Tests ein. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Vergleichsdiagramme.png}
    \caption{Vergleich der Modelle anhand des gemittelten F1 Scores über beide Klassen}
    \label{fig:vergleichsdiagramme}
\end{figure}

Die besten Ergebnisse liefert das Custom \ac{CNN}, während Mobilenet Version3 auch nach mehreren Optimierungsversuchen schlecht abschneidet.
Dies ist \autoref{fig:vergleichsdiagramme} zu entnehmen. Hier sind die gemittelten F1 Scores über beide Klassen für die verschiedenen Modelle dargestellt ist. Berechnungsgrundlage ist die in \autoref{sec:confusionmatrix} vorgestellte Confusion Matrix.

Ein detaillierter Vergleich der Modelle zeigt, dass das Custom \ac{CNN} auch in allen anderen Metriken überlegen ist. Aus Gründen der Übersichtlichkeit enfallen die genauen Tabellen mit allen Daten. Die Tabellen mit allen Werten sind dem Anhang \autoref{sec:performance_der_modelle} zu entnehmen. MobileNet V1 liefert dennoch gute Ergebnisse und ist aufgrund der Einfachheit der Integration und der guten Ergebnisse nicht zu vernachlässigen. 

Für die folgenden Betrachtungen wurd MobileNet V3 ausgeschlossen. Seine schlechten Ergebnisse lassen auf eine schlechte Anpassung an den Datensatz oder falsche Einstellung schließen.
Im Direkten Vergleich haben sowohl das Custom CNN als auch MobileNet V1 tendenziell mehr Probleme mit False Positive Klassifikationen. Dies ist in \autoref{fig:alle_cms} zu sehen.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{AlleCMs.png}
    \caption{Vergleich der Modelle anhand der Confusion Matrizen}
    \label{fig:alle_cms}
\end{figure}

Diese gemeinsamen Probleme lassen sich in diesem Fall auf den Datensatz zurückführen. Die falsch Positiv Klassifizierten Bilder sind ebenfalls für den Menschen nur schwer zu klassifizieren, da ein stück Leiterbahn fehlt oder eine schlecht belichtete Komponente fehlt (\ac{PCB}s siehe \autoref{fig:PCBs} Rechts). In den Praxistests während des Probebetriebs im Labor hat sich gezeigt, dass MobileNet V1 in allen beobachteten Fällen in der Lage ist eine korrekte Klassifikation durchzuführen.

Zusammenfassend lässt sich sagen, dass das Custom CNN in allen getesteten Metriken überlegen ist. Aufgrund der zeitlichen Einschränkungen und der bestehenden Programmpipeline wird jedoch weiterhin MobileNet V1 verwendet. Dieses Modell liefert gute Ergebnisse und ist einfacher zu integrieren.

